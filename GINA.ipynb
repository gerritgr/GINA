{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "thermal-point",
   "metadata": {},
   "source": [
    "# GINA\n",
    "### Neural Graph Inference From Independent Snapshots of Interacting System   \n",
    "Modeling and Simulation Group @ Saarland University \n",
    "\n",
    "https://arxiv.org/abs/2105.14329  \n",
    "https://github.com/gerritgr/gina  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "empirical-advisory",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "modified-satin",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pregnant-accused",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data_utils\n",
    "import math, random, os, time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import seaborn as sns\n",
    "sns.set_style(\"white\")\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import netrd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "structural-april",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "essential-ballot",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "HOME = './'   # where to save experiments output\n",
    "\n",
    "USE_VALIDATIONSET = True     # overwrites TRAINING_FRAC\n",
    "TRAINING_FRAC = 0.8          # only relveant when using test data\n",
    "LEARNING_RATE =  0.0001 \n",
    "TEMPERATURE = 5.0            # sharpness parameter v, higher temperature => sharper step function (mu)\n",
    "RANDOM_TEMPERATURE = False   # activates stochastic noise in step function (unused)\n",
    "INCREASE_TEMPERATURE = 3.0   # increase sharpness over course of training\n",
    "USE_LAYER_3 = False          # activate additional MLP layer\n",
    "INDIVIDUAL_INPUT = False     # actives custom embedding for each node (unused)\n",
    "INDIVIDUAL_OUTPUT = False    # custom output projection for each node\n",
    "MINIBATCH_SIZE = 500\n",
    "EPOCHNUM = 100*100\n",
    "OUTPUT_EACH_X_EPOCHS = 100   # how often evaluate GINA\n",
    "INIT_GRAPH_SHIFT = 1         # how to init graph (unused)\n",
    "USESHIFT = True\n",
    "COLLAPSE_IN_TEST = True      # only relevant when using training/test set\n",
    "TRAIN_NETWORK = True         # use this to fix graph during training and only train MLP weights\n",
    "EARLY_STOPPING = True        # only used in Exp3\n",
    "GRAPH_DROPOUT = True         # used to increase numerial stability\n",
    "\n",
    "print('Use  device: ', DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "under-petersburg",
   "metadata": {},
   "source": [
    "# Misc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "magnetic-belle",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faced-halifax",
   "metadata": {},
   "outputs": [],
   "source": [
    "def edgelist_to_graph(num_nodes, edgelist):\n",
    "  G = nx.Graph()\n",
    "  G.add_nodes_from(range(num_nodes))\n",
    "  G.add_edges_from(edgelist)\n",
    "  return G\n",
    "\n",
    "def float2str(x):\n",
    "  return str(x).zfill(20)\n",
    "\n",
    "def int2str(x):\n",
    "  return str(int(x+0.5)).zfill(20)\n",
    "\n",
    "def read_snapshots(filepath):\n",
    "  with open(filepath, 'r') as f:\n",
    "      lines = f.read().split('\\n')\n",
    "  states = [eval(line) for line in lines if len(line) > 2]\n",
    "  return states\n",
    "\n",
    "def write_snapshots(snapshots, filepath):\n",
    "  with open(filepath, 'w') as f:\n",
    "      for state in snapshots:\n",
    "          f.write(repr(state)+'\\n')\n",
    "\n",
    "\n",
    "def sample_temperature(mean_temperature, cut=3.0, var=0.2):\n",
    "    t = np.random.normal(mean_temperature, var)\n",
    "    if t < cut:\n",
    "        return cut\n",
    "    return t\n",
    "    \n",
    "def binarize_state(s, cut=None):\n",
    "    if len(s) == 2:\n",
    "        return s\n",
    "    if cut is None:\n",
    "        cut = int(len(s)/2+0.6)\n",
    "    return [np.sum(s[:cut]), np.sum(s[cut:])]\n",
    "\n",
    "def binarize_networkstate(networkstate, cut=None):\n",
    "    return [binarize_state(s, cut=cut) for s in networkstate]\n",
    "\n",
    "def binarize_snapshot(snapshot, cut=None):\n",
    "    return [binarize_networkstate(s, cut=cut) for s in snapshot]\n",
    "\n",
    "def snapshots_to_netrd(snapshots):\n",
    "  num_snapshots = len(snapshots)\n",
    "  network_size = len(snapshots[0])\n",
    "  snapshots_np = np.zeros([num_snapshots,network_size])\n",
    "  for i in range(num_snapshots):\n",
    "    for j in range(network_size):\n",
    "      snapshots_np[i][j] = np.argmax(snapshots[i][j])\n",
    "  snapshots_np = np.array(snapshots_np)\n",
    "  snapshots_np = snapshots_np.transpose()\n",
    "  return snapshots_np\n",
    "\n",
    "\n",
    "def nx_to_adj(G):\n",
    "  a_ij = nx.adjacency_matrix(G).todense()\n",
    "  a_ij = (a_ij > 0.0001).astype('float')\n",
    "  a_ij = torch.FloatTensor(a_ij)\n",
    "  a_ij = a_ij.to(DEVICE)\n",
    "  return a_ij\n",
    "\n",
    "def graph_dist(G1, G2):\n",
    "  a_ij = nx.adjacency_matrix(G1).todense() # graphs have to be symmetric\n",
    "  b_ij = nx.adjacency_matrix(G2).todense()\n",
    "\n",
    "  #threshold\n",
    "  a_ij = (a_ij > 0.0001).astype('float')\n",
    "  b_ij = (b_ij > 0.0001).astype('float')\n",
    "\n",
    "  z_ij = a_ij - b_ij\n",
    "  z_ij = np.abs(z_ij)\n",
    "  np.fill_diagonal(z_ij, 0.0) # should not need this\n",
    "  return np.sum(z_ij)/2\n",
    "\n",
    "def gen_folders(exp_name, home=HOME):\n",
    "    os.system('mkdir '+home)\n",
    "    os.system('mkdir '+home+exp_name)\n",
    "    if not os.path.exists(home+exp_name+'/'+'NeuralNetworkReconstruction.ipynb'):\n",
    "        orig_path = \"NeuralNetworkReconstruction.ipynb\"\n",
    "    copy_path = home+exp_name+'/'+'NeuralNetworkReconstruction.ipynb'\n",
    "    os.system(\"cp '{}' '{}'\".format(orig_path, copy_path))\n",
    "    print(\"cp '{}' '{}'\".format(orig_path, copy_path))\n",
    "    for out_folder in ['graph_evol', 'snapshots', 'weights', 'dynamics_prediction']:\n",
    "        os.system('mkdir '+home+exp_name+'/'+out_folder)\n",
    "\n",
    "# this needs to be tested more\n",
    "def clean_shuffle_graph(G):\n",
    "  random_seed_state =  int(random.random()*100000) # quick hack to go back to random afterwards\n",
    "  random.seed(42)\n",
    "  node_mapping = dict(zip(sorted(G.nodes()), sorted(G.nodes(), key=lambda _: random.random()))) # maybe sorted not really deterministic\n",
    "  G = nx.relabel_nodes(G, node_mapping)\n",
    "  G = nx.convert_node_labels_to_integers(G)\n",
    "  if not nx.is_connected(G):\n",
    "    print('Graph is not connected, try a differnt one.')\n",
    "    assert(nx.is_connected(G))\n",
    "  random.seed(random_seed_state)\n",
    "  return G\n",
    "\n",
    "\n",
    "def split_snapshots(snapshots, training_frac=TRAINING_FRAC):\n",
    "  if not USE_VALIDATIONSET:\n",
    "    training_frac = 1.0\n",
    "  random.seed(42)\n",
    "  random.shuffle(snapshots)\n",
    "  cut_off = int(len(snapshots)*training_frac)\n",
    "  training_data = snapshots[:cut_off]\n",
    "  test_data = snapshots[cut_off:]\n",
    "  random.seed(None)\n",
    "  training_data = [torch.tensor(snapshot, dtype=torch.float, device=DEVICE) for snapshot in training_data]\n",
    "  test_data = [torch.tensor(snapshot, dtype=torch.float, device=DEVICE) for snapshot in test_data]\n",
    "  if not USE_VALIDATIONSET:\n",
    "    return training_data, None\n",
    "  return training_data, test_data\n",
    "\n",
    "def split_single_snapshot(snapshot, pos = None):\n",
    "  snapshot = snapshot.clone()\n",
    "  network_size = snapshot.shape[0]\n",
    "  state_num = snapshot.shape[1]\n",
    "  if pos is None:\n",
    "    pos = random.choice(range(network_size))\n",
    "  ground_truth = snapshot[pos,:].clone()\n",
    "  snapshot[pos,:] = 0.0\n",
    "  return snapshot, ground_truth, pos\n",
    "\n",
    "\n",
    "def state_to_color(state):\n",
    "  colors = ['blue', 'red', 'black', 'green', 'orange', 'yellow', 'pink']\n",
    "  return colors[np.argmax(state) % 6]\n",
    "\n",
    "def plot_graph(G, name, exp_name, ground_truth=None, is_final=False):\n",
    "  plt.clf()\n",
    "  if ground_truth is not None:\n",
    "    pos=nx.kamada_kawai_layout(ground_truth)\n",
    "  else:\n",
    "    pos=nx.kamada_kawai_layout(G)\n",
    "  nx.draw(G, node_color='black', pos=pos, alpha=0.5)\n",
    "  plt.savefig(HOME+exp_name+'/graph_evol/{}.png'.format(name))\n",
    "  if is_final:\n",
    "    nx.write_gml(G, HOME+exp_name+'/final_graph.gml'.format(name))\n",
    "    plt.savefig(HOME+exp_name+'/graph_evol/final_graph.png'.format(name))\n",
    "\n",
    "\n",
    "def plot_dynamics(name, exp_name, model, num=10, state_num=2):\n",
    "  im = np.zeros([num, num])\n",
    "  for i in range(num):\n",
    "    for j in range(num):\n",
    "      input = np.ones(state_num)\n",
    "      input[0] = i\n",
    "      input[1] = j\n",
    "      output = model.forward_counts(torch.tensor(input, dtype=torch.float, device=DEVICE))\n",
    "      output = output.view(-1)\n",
    "      im[i,j] = float(output[0])\n",
    "  plt.clf()\n",
    " # plt.imshow(im)\n",
    "  ax = sns.heatmap(im, vmin=0, vmax=1, cmap='vlag', square=True)\n",
    "  plt.ylim(0,num)\n",
    "  plt.xlim(0,num)   \n",
    "  plt.savefig(HOME+exp_name+'/dynamics_prediction/{}.png'.format(name))\n",
    "\n",
    "\n",
    "def plot_snapshots(G, snapshots, exp_name, max_plot_num=30):\n",
    "  print(\"plot_snapshots\")\n",
    "  for i, snapshot in enumerate(snapshots):\n",
    "    assert(len(snapshot) == G.number_of_nodes())\n",
    "    if max_plot_num is not None and i>=max_plot_num:\n",
    "      break\n",
    "    node_color = [state_to_color(snapshot[j]) for j in range(len(snapshot))]\n",
    "    plt.clf()\n",
    "    nx.draw(G, node_color=node_color, pos=nx.spectral_layout(G))\n",
    "    plt.savefig(HOME+exp_name+'/snapshots/snapshot_{}.png'.format(int2str(i)))\n",
    "    plt.clf()\n",
    "    nx.draw(G, node_color=node_color, pos=nx. kamada_kawai_layout(G))\n",
    "    plt.savefig(HOME+exp_name+'/snapshots/l_snapshot_{}.png'.format(int2str(i)))\n",
    "\n",
    "\n",
    "def datasets_form_snapshots(snapshots, training_frac=TRAINING_FRAC):\n",
    "  # 2 slow 2 use\n",
    "    random.seed(42)\n",
    "    random.shuffle(snapshots)\n",
    "    cut_off = int(len(snapshots)*training_frac)\n",
    "    training_data = snapshots[:cut_off]\n",
    "    test_data = snapshots[cut_off:]\n",
    "    random.seed(None)\n",
    "    \n",
    "    network_size = len(snapshots[0])\n",
    "    \n",
    "    train = np.array(training_data)\n",
    "    train = train.reshape([-1,network_size])\n",
    "    train = torch.FloatTensor(train, device=DEVICE)\n",
    "    train_loader = data_utils.DataLoader(train, batch_size=MINIBATCH_SIZE, shuffle=True)\n",
    "\n",
    "    test = np.array(test_data)\n",
    "    test = test.reshape([-1,network_size])\n",
    "    test = torch.FloatTensor(test, device=DEVICE)\n",
    "    test_loader = data_utils.DataLoader(test, batch_size=test.shape[0])\n",
    "    \n",
    "    return train_loader, test_loader\n",
    "\n",
    "def fill_symmetric_matrix(values, node_num):\n",
    "  assert(len(values) == node_num*(node_num-1)/2)\n",
    "  m = torch.zeros([node_num,node_num], dtype=torch.double, device=DEVICE)\n",
    "  counter = -1\n",
    "  for i in range(m.shape[0]):\n",
    "    for j in range(m.shape[1]):\n",
    "      if i<j:\n",
    "        counter += 1\n",
    "        m[i,j] = values[counter]\n",
    "        m[j,i] = values[counter]\n",
    "  return m\n",
    "  \n",
    "def create_all_graphs(node_num, check_if_connected=True):\n",
    "  import itertools\n",
    "  len_values = int(node_num*(node_num-1)/2)\n",
    "  graphs = list(itertools.product([0.0,1.0], repeat=len_values))\n",
    "  graphs = [fill_symmetric_matrix(g, node_num) for g in graphs]\n",
    "  graphs = [nx.from_numpy_matrix(g.cpu().detach().numpy()) for g in graphs]\n",
    "  if check_if_connected:\n",
    "    graphs = [g for g in graphs if nx.is_connected(g)]\n",
    "  return graphs\n",
    "\n",
    "class SnapshotLoader(): # todo make to iterator\n",
    "    def __init__(self, snapshots, batch_size=None, shuffle=True):\n",
    "        self.snapshots = [torch.FloatTensor(snapshot, device=DEVICE) for snapshot in snapshots]\n",
    "        if batch_size is None:\n",
    "          batch_size = len(snapshots)\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "    def get_data(self):\n",
    "      if self.shuffle:\n",
    "        random.shuffle(self.snapshots)\n",
    "      data_list = [list()]\n",
    "      for i, snapshot in enumerate(self.snapshots):\n",
    "        if len(data_list[-1]) >= self.batch_size:\n",
    "          data_list.append(list())\n",
    "        data_list[-1].append(snapshot)\n",
    "      data_list = [torch.cat(test_data, dim=1) for test_data in data_list]\n",
    "      return data_list\n",
    "\n",
    "def snapshots_to_loader(snapshots, training_frac=TRAINING_FRAC):\n",
    "    random.seed(42)\n",
    "    random.shuffle(snapshots)\n",
    "    cut_off = int(len(snapshots)*training_frac)\n",
    "    training_data = snapshots[:cut_off]\n",
    "    test_data = snapshots[cut_off:]\n",
    "    random.seed(None)\n",
    "\n",
    "    training_loader = SnapshotLoader(training_data, batch_size=MINIBATCH_SIZE)\n",
    "    test_loader = SnapshotLoader(test_data, shuffle=False)\n",
    "    return training_loader, test_loader\n",
    "\n",
    "def index_set(l, minibatch_size=MINIBATCH_SIZE):\n",
    "    v = list()\n",
    "    start = 0\n",
    "    while l>0:\n",
    "        cut = min(l,minibatch_size)\n",
    "        v.append((start,start+cut))\n",
    "        l -= cut\n",
    "        start += cut\n",
    "    return v\n",
    "\n",
    "def plot_atlas(outpath, df):\n",
    "  # 'loss_train': list(), 'loss_test': list(), 'acc_test': list()\n",
    "  plt.clf()\n",
    "  plt.close()\n",
    "  f, axes = plt.subplots(3, 1)\n",
    "  sns.violinplot(data=df ,x='graphdist', y='loss_train', inner=\"stick\", ax=axes[0], cut=0)\n",
    "  sns.violinplot(data=df ,x='graphdist', y='loss_test', inner=\"stick\", ax=axes[1], cut=0)\n",
    "  sns.violinplot(data=df ,x='graphdist', y='acc_test', inner=\"stick\", ax=axes[2], cut=0)\n",
    "  plt.title('loss_train loss_test acc_test')\n",
    "  plt.savefig(outpath)\n",
    "  plt.clf()\n",
    "  f, axes = plt.subplots(3, 1)\n",
    "  sns.scatterplot(data=df ,x='graphdist', y='loss_train',  ax=axes[0], alpha=0.5)\n",
    "  sns.scatterplot(data=df ,x='graphdist', y='loss_test', ax=axes[1], alpha=0.5)\n",
    "  sns.scatterplot(data=df ,x='graphdist', y='acc_test',  ax=axes[2], alpha=0.5)\n",
    "  plt.title('loss_train loss_test acc_test')\n",
    "  plt.savefig(outpath.replace('.pdf', '_scatter.pdf'))\n",
    "\n",
    "\n",
    "def  plot_exp2(outpath, df):\n",
    "  plt.clf()\n",
    "  plt.close()\n",
    "    \n",
    "  sns.lineplot(data= df[df.grid_dim.eq(5)] ,x='epoch', y='graphdist', hue=\"sample_num\", palette='bright')\n",
    "  plt.savefig(outpath.replace('.pdf', '_03.pdf'))\n",
    "  plt.clf()\n",
    "              \n",
    "  sns.lineplot(data= df[df.grid_dim.eq(7)] ,x='epoch', y='graphdist', hue=\"sample_num\", palette='bright')\n",
    "  plt.savefig(outpath.replace('.pdf', '_07.pdf'))            \n",
    "  plt.clf()\n",
    "              \n",
    "  sns.lineplot(data= df[df.grid_dim.eq(10)] ,x='epoch', y='graphdist', hue=\"sample_num\", palette='bright')\n",
    "  plt.savefig(outpath.replace('.pdf', '0_10.pdf'))\n",
    "    \n",
    "def adj_size(n):\n",
    "    return n*(n-1)/2.0\n",
    "\n",
    "def plot_exp3(df, outpath):\n",
    "    for dynamicsname in sorted(list(set(df.dynamicsname))):\n",
    "        df_i = df[df.dynamicsname == dynamicsname]\n",
    "        df_i['adj_size'] = df_i.apply(lambda x: adj_size(x['node_num']), axis = 1) \n",
    "        df_i['acc'] = df_i.apply(lambda x: (x['adj_size']-x['graphdist'])/x['adj_size'], axis = 1)\n",
    "        sns.catplot(x='graphname', y='acc', hue='method', data=df_i, kind=\"bar\")\n",
    "        plt.savefig(outpath.format(dynamicsname))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "loose-illustration",
   "metadata": {},
   "source": [
    "## Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "grave-stable",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_threshold(datalist):\n",
    "  from sklearn.cluster import KMeans\n",
    "  datalist = sorted(datalist)\n",
    "  datalist_2d = [[l] for l in datalist]\n",
    "  kmeans = KMeans(n_clusters=2, random_state=0).fit(datalist_2d)\n",
    "  labels = list(kmeans.labels_)\n",
    "  num_clust1 = list(kmeans.labels_).count(labels[0])\n",
    "  return datalist[num_clust1-1]+0.000000000001\n",
    "\n",
    "\n",
    "def check_baseline_autothreshold(snapshots, recon, ground_truth_graph):\n",
    "  import netrd\n",
    "  TS = snapshots_to_netrd(snapshots)\n",
    "  #G = ground_truth_graph\n",
    "\n",
    "  #avg_k = np.mean([G.degree(n) for n in G.nodes()])\n",
    "  avg_k = 5 # value should not matter\n",
    "  #recon = netrd.reconstruction.MutualInformationMatrix()\n",
    "  G_pred = recon.fit(TS, threshold_type='degree', avg_k = avg_k)\n",
    "  W = recon.results['weights_matrix']\n",
    "  W = W + W.transpose()\n",
    "  np.fill_diagonal(W, 0.0)\n",
    "  Wx = W[np.triu_indices(W.shape[0])]\n",
    "  Wx = Wx.flatten()\n",
    "  t = find_threshold(list(Wx)) \n",
    "  print('treshold: ',t, ' min ', np.min(Wx), ' max ', np.max(Wx))\n",
    "  a_ij = (W > t).astype('float')\n",
    "\n",
    "  gt_ij = nx.adjacency_matrix(ground_truth_graph).todense()\n",
    "  gt_ij = (gt_ij > 0.0001).astype('float')\n",
    "\n",
    "  z_ij = a_ij - gt_ij\n",
    "  z_ij = np.abs(z_ij)\n",
    "  np.fill_diagonal(z_ij, 0.0) # should not need this\n",
    "  return np.sum(z_ij)/2, -1.0\n",
    "\n",
    "\n",
    "def chec_spec_baseline(snapshots, ground_truth_graph, recon):\n",
    "  TS = snapshots_to_netrd(snapshots)\n",
    "  avg_k = np.mean([ground_truth_graph.degree(n) for n in ground_truth_graph.nodes()])\n",
    "  #recon = netrd.reconstruction.GraphicalLasso()\n",
    "  start_time = time.time() \n",
    "  G_pred = recon.fit(TS, threshold_type='degree', avg_k = avg_k)\n",
    "  return graph_dist(ground_truth_graph, G_pred), time.time()-start_time\n",
    "\n",
    "def baseline_summary(snapshots, ground_truth_graph):\n",
    "  import netrd\n",
    "  results = dict()\n",
    "  baseline_methods = {#'ConvergentCrossMapping': netrd.reconstruction.ConvergentCrossMapping(),\n",
    "           'CorrelationMatrix': netrd.reconstruction.CorrelationMatrix(),\n",
    "          # 'CorrelationSpanningTree': netrd.reconstruction.CorrelationSpanningTree(),\n",
    "           #'FreeEnergyMinimization': netrd.reconstruction.FreeEnergyMinimization(),\n",
    "           #'GrangerCausality': netrd.reconstruction.GrangerCausality(),\n",
    "           'GraphicalLasso': netrd.reconstruction.GraphicalLasso(),\n",
    "           #'MarchenkoPastur': netrd.reconstruction.MarchenkoPastur(),\n",
    "           #'MaximumLikelihoodEstimation': netrd.reconstruction.MaximumLikelihoodEstimation(),\n",
    "           #'MeanField': netrd.reconstruction.MeanField(),\n",
    "           'MutualInformationMatrix': netrd.reconstruction.MutualInformationMatrix(),\n",
    "           #'NaiveTransferEntropy': netrd.reconstruction.NaiveTransferEntropy(),\n",
    "           #'OptimalCausationEntropy': netrd.reconstruction.OptimalCausationEntropy(),\n",
    "           #'PartialCorrelationInfluence': netrd.reconstruction.PartialCorrelationInfluence()}\n",
    "           'PartialCorrelationMatrix': netrd.reconstruction.PartialCorrelationMatrix()}\n",
    "           #'OUInference': netrd.reconstruction.OUInference()}\n",
    "           #'ThoulessAndersonPalmer': netrd.reconstruction.ThoulessAndersonPalmer()\n",
    "        \n",
    "  for name, recon in baseline_methods.items():\n",
    "    dist = -1.0\n",
    "    try:\n",
    "      dist, time = chec_spec_baseline(snapshots, ground_truth_graph, recon)\n",
    "      print(name, dist, time)\n",
    "    except:\n",
    "      pass\n",
    "    results[name] = (dist, time)\n",
    "    # auto threshold\n",
    "    dist_auto = -1.0\n",
    "    time_auto = -1\n",
    "    # add baseline\n",
    "    #try:\n",
    "    #  dist_auto, time_auto = check_baseline_autothreshold(snapshots, recon,ground_truth_graph)\n",
    "    #except:\n",
    "    #  pass\n",
    "    #results[name+'_auto'] = (dist_auto, time_auto)\n",
    "  return results\n",
    "    \n",
    "\n",
    "def check_baseline(snapshots, ground_truth_graph):\n",
    "  import netrd\n",
    "  TS = snapshots_to_netrd(snapshots)\n",
    "  G = ground_truth_graph\n",
    "\n",
    "  avg_k = np.mean([G.degree(n) for n in G.nodes()])\n",
    "  recon = netrd.reconstruction.GraphicalLasso()\n",
    "  G_pred = recon.fit(TS, threshold_type='degree', avg_k = avg_k)\n",
    "  recon = netrd.reconstruction.MutualInformationMatrix()\n",
    "  G_pred_witg = recon.fit(TS, threshold_type='degree', avg_k = avg_k)\n",
    "  return graph_dist(G, G_pred), graph_dist(G, G_pred_witg)\n",
    "\n",
    "def gen_init_graph(snapshots):\n",
    "  import netrd\n",
    "  TS = snapshots_to_netrd(snapshots)\n",
    "  recon = netrd.reconstruction.MutualInformationMatrix()\n",
    "  G_pred = recon.fit(TS, threshold_type='quantile', quantile = 0.7)\n",
    "  a_ij = nx.adjacency_matrix(G_pred).todense()\n",
    "  a_ij = (a_ij > 0.0001).astype('float')\n",
    "  a_ij = torch.FloatTensor(a_ij)\n",
    "  a_ij = a_ij.to(DEVICE)\n",
    "  return a_ij"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "proper-genre",
   "metadata": {},
   "source": [
    "# PyTorch Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "arbitrary-burner",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "class MyNetworkLayer(nn.Module):\n",
    "    \"\"\" Custom Network layer \"\"\"\n",
    "    def __init__(self, network_size, state_num=2, init_graph = None, train_network = TRAIN_NETWORK):\n",
    "        super().__init__()\n",
    "        self.network_size = network_size\n",
    "        self.state_num = state_num\n",
    "        self.size_out = state_num # 2 local states S and I\n",
    "        self.temperature = TEMPERATURE\n",
    "        weights = torch.Tensor(self.network_size, self.network_size)\n",
    "        self.weights = nn.Parameter(weights)  # nn.Parameter is a Tensor that's a module parameter.\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "        self.weight_shift = nn.Parameter(torch.FloatTensor([0.0]))\n",
    "        if not USESHIFT:\n",
    "          self.weight_shift.requires_grad = False\n",
    "        self.thresholding = None\n",
    "        if True:\n",
    "          temperature = self.temperature\n",
    "          if RANDOM_TEMPERATURE:\n",
    "            temperature = sample_temperature(self.temperature)\n",
    "          self.thresholding = lambda x: self.sigmoid((self.sigmoid(x)-0.5) * temperature)   \n",
    "        else:\n",
    "          self.thresholding = torch.nn.Sigmoid()\n",
    "\n",
    "        nn.init.kaiming_uniform_(self.weights, a=math.sqrt(5)) # weight init\n",
    "        self.dropout = nn.Dropout(p=0.005)\n",
    "        \n",
    "        self.random_noise = list(np.linspace(0.1,0.025,int(EPOCHNUM/1.0))) + [0.0]*EPOCHNUM\n",
    "\n",
    "        if init_graph is not None:\n",
    "          init_graph = (init_graph - 0.5)*INIT_GRAPH_SHIFT\n",
    "          self.weights = nn.Parameter(init_graph)\n",
    "        if not TRAIN_NETWORK:\n",
    "          self.weights.requires_grad = False\n",
    "          self.weight_shift.requires_grad = False\n",
    "\n",
    "    def diag_graph_matrix(self, step_i):\n",
    "        m = self.thresholding(self.weights-self.weight_shift)  \n",
    "        m = torch.triu(m)\n",
    "        m = (m + m.t())\n",
    "        #m = self.thresholding(m) \n",
    "        m.fill_diagonal_(0.0) #  TODO use diagonal=1 instead at triu(m)\n",
    "        #epoch_i = 0\n",
    "        if GRAPH_DROPOUT:\n",
    "            noise = self.random_noise[step_i]\n",
    "            m = m + (torch.randn(m.shape)*noise).to(DEVICE)\n",
    "            #m = self.dropout(m)\n",
    "        return m\n",
    "    \n",
    "    def increase_temperature(self, step_i, increase=None):\n",
    "        #if self.temperature > 20:\n",
    "        #    return\n",
    "        increase = INCREASE_TEMPERATURE if increase is None else float(increase)\n",
    "        self.temperature += increase \n",
    "\n",
    "    def forward(self, x, step_i=0, graph_collapse = False):\n",
    "        adj_m = self.diag_graph_matrix(step_i)\n",
    "        if graph_collapse:\n",
    "          adj_m = (adj_m > 0.5).float()\n",
    "        env_count = adj_m.mm(x)\n",
    "        env_count = env_count.view(-1,self.state_num) \n",
    "        return env_count\n",
    "\n",
    "    def collapse(self, stochastic=True):\n",
    "      # TODO fix\n",
    "      with torch.no_grad():\n",
    "        weights = self.weights\n",
    "        weights_new = torch.zeros(weights.shape)\n",
    "\n",
    "        for i in range(weights.shape[0]):\n",
    "          for j in range(weights.shape[1]):\n",
    "            if i==j:\n",
    "              weights_new[i,j] = 0.0\n",
    "              continue\n",
    "            if i<j:\n",
    "              w_ij = weights[i,j]\n",
    "            else:\n",
    "              w_ij = weights[j,i]\n",
    "            if stochastic:\n",
    "              cut_off = np.random.random()\n",
    "            weights_new[i,j] = 0.1 if w_ij > 0.0 else -0.1\n",
    "\n",
    "        for i in range(weights.shape[0]):\n",
    "          for j in range(weights.shape[1]):\n",
    "            self.weights[i,j] = weights_new[i,j]\n",
    "\n",
    "    def threshold_egelist(self):\n",
    "      edgelist = list()\n",
    "      with torch.no_grad():\n",
    "        weights = self.diag_graph_matrix(EPOCHNUM-1)\n",
    "        for i in range(weights.shape[0]):\n",
    "          for j in range(weights.shape[1]):\n",
    "            if i>j:\n",
    "              continue \n",
    "            if weights[i,j] > 0.5:\n",
    "              edgelist.append([i, j])\n",
    "      return edgelist\n",
    "\n",
    "    def threshold_nx(self):\n",
    "      import networkx as nx\n",
    "      edgelist = self.threshold_egelist()\n",
    "      G = nx.Graph()\n",
    "      G.add_nodes_from(range(self.network_size))\n",
    "      G.add_edges_from(edgelist)\n",
    "      return G\n",
    "\n",
    "    def set_graph(self, edge_list):\n",
    "      with torch.no_grad():\n",
    "        for i in range(weights.shape[0]):\n",
    "          for j in range(weights.shape[1]):\n",
    "            self.weights[i,j] = -3.0 if i != j else 0.0\n",
    "          \n",
    "        for i, j in edge_list:\n",
    "          self.weights[i,j] = 3.0\n",
    "          self.weights[j,i] = 3.0\n",
    "\n",
    "\n",
    "class MyGraphNetwork(nn.Module):\n",
    "    \"\"\" Custom Network layer \"\"\"\n",
    "    def __init__(self, network_size = 4, state_num=2, init_graph=None):\n",
    "        super().__init__()\n",
    "        self.state_num = state_num\n",
    "        self.network_size = network_size\n",
    "        state_num_latent = state_num\n",
    "        if INDIVIDUAL_INPUT:\n",
    "            state_num_latent *= 2\n",
    "        self.state_num_latent = state_num_latent\n",
    "        self.dropout = nn.Dropout(p=0.05)\n",
    "        \n",
    "        #Create Network Layer\n",
    "        self.network_push = MyNetworkLayer(network_size,state_num=state_num_latent, init_graph=init_graph)\n",
    "        \n",
    "        # Fully Connected\n",
    "        self.fc1 = nn.Linear(state_num_latent, 10)\n",
    "        self.fc2 = nn.Linear(10, 10)\n",
    "        self.fc3 = nn.Linear(10, 10)  #optional\n",
    "        self.fc4 = nn.Linear(10, state_num)\n",
    "        if INDIVIDUAL_OUTPUT:\n",
    "          individual_output_weight = torch.eye(state_num)\n",
    "          individual_output_weight = individual_output_weight.reshape((1, state_num, state_num))\n",
    "          individual_output_weight =  individual_output_weight.repeat(network_size, 1, 1)\n",
    "          self.individual_output_weight = nn.Parameter(individual_output_weight) \n",
    "        if INDIVIDUAL_INPUT:\n",
    "          individual_input_weight = torch.Tensor(state_num, state_num_latent)\n",
    "          individual_input_weight = individual_input_weight.reshape((1, state_num, state_num_latent))\n",
    "          individual_input_weight =  individual_input_weight.repeat(network_size, 1, 1)\n",
    "          self.individual_input_weight = nn.Parameter(individual_input_weight)\n",
    "          nn.init.kaiming_uniform_(self.individual_input_weight, a=math.sqrt(5))\n",
    "            \n",
    "            \n",
    "    def forward(self, x, step_i=0, graph_collapse=False):\n",
    "        \n",
    "        # Prepare Network pass\n",
    "        if INDIVIDUAL_INPUT:\n",
    "          x = self.compute_individual_input(x)\n",
    "\n",
    "        # Network Pass\n",
    "        x =  self.network_push(x, step_i, graph_collapse=graph_collapse)\n",
    "        # MLP Pass\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(x)\n",
    "        #x = self.dropout(x)\n",
    "        if USE_LAYER_3:\n",
    "            x = self.fc3(x)\n",
    "            x = F.relu(x)\n",
    "        x = self.fc4(x)\n",
    "\n",
    "        output = F.softmax(x, dim=1)\n",
    "        \n",
    "        if INDIVIDUAL_OUTPUT and self.network_push.temperature > 0:\n",
    "          # version 1\n",
    "          #sample_num = x.shape[0]\n",
    "          #repeat = int(sample_num/self.network_size)\n",
    "          #sample_range = list(range(self.network_size)) * repeat\n",
    "          #x_new = x.clone().detach()\n",
    "          #for node_i in range(self.network_size):\n",
    "          #  node_i_mask = [1 if i == node_i else 0 for i in sample_range]\n",
    "          #  weight_i = self.individual_output_weight[node_i,:,:]\n",
    "          #  x_new[node_i_mask,:] = x[node_i_mask,:].matmul(weight_i)\n",
    "          #output = F.softmax(x, dim=1)\n",
    "            \n",
    "          # version 2\n",
    "          line_num = int(x.shape[0])\n",
    "          snapshot_num = int(line_num/self.network_size) \n",
    "          #print('line_num ', line_num, ' snapshot_num ',snapshot_num, '  output shape ', output.shape)\n",
    "          individual_output_weight_r = self.individual_output_weight.repeat(snapshot_num,1,1)\n",
    "          #print('individual_output_weight_r ',individual_output_weight_r.shape)\n",
    "          output = output.view(line_num,1,self.state_num)\n",
    "          output = output.matmul(individual_output_weight_r).view(line_num,self.state_num)\n",
    "          output = F.softmax(output, dim=1)\n",
    "        return output\n",
    "\n",
    "    def compute_individual_input(self, x):\n",
    "      x = x.view(-1, self.state_num)\n",
    "      line_num = int(x.shape[0])\n",
    "      snapshot_num = int(line_num/self.network_size) \n",
    "      individual_input_weight_r = self.individual_input_weight.repeat(snapshot_num,1,1)\n",
    "      x = x.view(line_num,1,self.state_num)\n",
    "      x = x.matmul(individual_input_weight_r)\n",
    "      #x = x.view(line_num,self.state_num_latent)\n",
    "      #x = F.relu(x)\n",
    "      x = x.view(-1,self.state_num)\n",
    "      x  =  F.softmax(x, dim=1)\n",
    "      x = x.view(self.network_size,-1)\n",
    "      return x\n",
    "    \n",
    "    # for debugging\n",
    "    def forward_counts(self, x):\n",
    "      with torch.no_grad():\n",
    "        x = x.view([1,self.state_num])\n",
    "        if INDIVIDUAL_INPUT:\n",
    "            #x = x.view(-1,self.state_num)\n",
    "            #x = self.compute_individual_input(x) #stupid hack, not really valid\n",
    "            #x = x.view([1,self.state_num])\n",
    "            return torch.zeros(x.shape)\n",
    "        # MLP Pass\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(x)\n",
    "        if USE_LAYER_3:\n",
    "          x = self.fc3(x)\n",
    "          x = F.relu(x)\n",
    "            \n",
    "        x = self.fc4(x)\n",
    "        output = F.softmax(x, dim=1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fitted-mentor",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sophisticated-indication",
   "metadata": {},
   "source": [
    "### Training Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sufficient-ensemble",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_net_new(model, test_loader):\n",
    "  criterion = nn.MSELoss()\n",
    "  model.eval()\n",
    "  with torch.no_grad():\n",
    "    acc_list = list()\n",
    "    loss_list = list()\n",
    "    for snapshots in test_loader.get_data(): # make sure there is only one\n",
    "      output = model(snapshots, graph_collapse=COLLAPSE_IN_TEST)\n",
    "      ground_truth = snapshots.view(-1,model.state_num)\n",
    "      loss_i = float(criterion(output, ground_truth))\n",
    "      acc_i = torch.argmax(output, dim=1) == torch.argmax(ground_truth, dim=1)\n",
    "      acc_i = torch.sum(acc_i) / acc_i.shape[0]\n",
    "      model.train()\n",
    "      return float(acc_i), loss_i\n",
    "\n",
    "def test_net(model, test_data):\n",
    "  criterion = nn.MSELoss()\n",
    "  model.eval()\n",
    "  with torch.no_grad():\n",
    "    acc_list = list()\n",
    "    loss_list = list()\n",
    "    snapshots = torch.cat(test_data, dim=1)\n",
    "    output = model(snapshots, graph_collapse=True)\n",
    "    ground_truth = snapshots.view(-1,model.state_num)\n",
    "    loss_i = float(criterion(output, ground_truth))\n",
    "    acc_i = torch.argmax(output, dim=1) == torch.argmax(ground_truth, dim=1)\n",
    "    acc_i = float(torch.sum(acc_i) / acc_i.shape[0])\n",
    "    model.train()\n",
    "    return acc_i, loss_i\n",
    "\n",
    "def load_model(model, exp_name):\n",
    "  import glob\n",
    "  weight_list = glob.glob(HOME+exp_name+'/weights/train_*.weight')\n",
    "  if len(weight_list) == 0:\n",
    "    print('start without init weights')\n",
    "    return None, 0\n",
    "  weight_list = sorted(weight_list)\n",
    "  weight_path = weight_list[-1]\n",
    "  print('load: '+weight_path)\n",
    "  model.load_state_dict(torch.load(weight_path))\n",
    "\n",
    "  record_df = pd.read_csv(HOME+exp_name+'/summary.csv', sep=';')\n",
    "  start_shift = list(record_df['epoch'])[-1]\n",
    "\n",
    "  return record_df, int(start_shift)\n",
    "\n",
    "\n",
    "def start_training(*args, **kwargs):\n",
    "  import pickle\n",
    "  exp_name = args[0]\n",
    "  assert(\"<class 'str'>\" in str(type(exp_name)))\n",
    "\n",
    "  try:\n",
    "    filepath = HOME+exp_name+'/result.pickel'\n",
    "    return_values = pickle.load(open(filepath, \"rb\" ))\n",
    "    print('read results: ',filepath)\n",
    "    return return_values\n",
    "  except:\n",
    "    return_values = training(*args, **kwargs)\n",
    "    filepath = HOME+exp_name+'/result.pickel'\n",
    "    pickle.dump(return_values, open(filepath, \"wb\" ))\n",
    "    return return_values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "common-battlefield",
   "metadata": {},
   "source": [
    "### Training Rounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "natural-balance",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(exp_name, snapshots, ground_truth_graph=None, state_num=2, init_graph=None):\n",
    "  from torch.optim import Adam\n",
    "  assert(len(snapshots[0][0]) == state_num) # is model.state_num\n",
    "  network_size = len(snapshots[0])\n",
    "  lr = LEARNING_RATE #0.0005 #0.008 # 0.0001  # 0.001\n",
    "  epochs = EPOCHNUM \n",
    "  training_snapshots, test_snapshots = split_snapshots(snapshots) # old\n",
    "  #train_loader, test_loader = snapshots_to_loader(snapshots) #new\n",
    "  model = MyGraphNetwork(network_size=network_size, state_num=state_num, init_graph=init_graph)\n",
    "  model = model.to(DEVICE)\n",
    "  record_df, start_shift = load_model(model, exp_name)\n",
    "  optimizer = Adam(model.parameters(), lr = lr) \n",
    "  criterion = nn.MSELoss()\n",
    "  if record_df is None:\n",
    "    record_df = {'epoch': list(), 'loss_train': list(), 'loss_test': list(), 'acc_test': list(), 'graph_edges': list(), 'weightpath': list(), 'graphdist': list()}\n",
    "    record_df = pd.DataFrame(record_df)\n",
    "\n",
    "  if ground_truth_graph is not None:\n",
    "    plot_graph(ground_truth_graph, 'ground_truth', exp_name=exp_name)\n",
    "\n",
    "  start_time = time.time() # measure time per epoch\n",
    "  forward_count = 0\n",
    "  best_mean_test_acc_sofar = -1.0\n",
    "  decr_epoch_count = 0\n",
    "  best_graphdiff_sofar = 0\n",
    "  processed_samples = 0\n",
    "  overall_loss = 0.0\n",
    "  graph_epoch_lists = [list() for _ in range(10)] # start with empty graphs \n",
    "\n",
    "  for step_j in range(epochs):\n",
    "    step_i = step_j+start_shift\n",
    "    if step_i >= EPOCHNUM:\n",
    "        break\n",
    "    weightpath = HOME+exp_name+'/weights/train_{}.weight'\n",
    "    loss_sum = torch.tensor([0.0], requires_grad=True, dtype=torch.float, device=DEVICE)\n",
    "    random.shuffle(training_snapshots)  # old\n",
    "    epoch_loss = list()\n",
    "    for cut in index_set(len(training_snapshots)):\n",
    "      #cut = min(MINIBATCH_SIZE, len(training_snapshots))  # old\n",
    "      #trainings_data = train_loader.get_data() #new\n",
    "      #for snapshot_minibatch in trainings_data:  #new\n",
    "      if step_i == 0: # to make sure 1st evaluation is based on untrained weight matrix\n",
    "        break\n",
    "            \n",
    "      snapshot_minibatch = torch.cat(training_snapshots[cut[0]:cut[1]], dim=1) #old\n",
    "      model.zero_grad()\n",
    "      processed_samples += cut[1] - cut[0]  #int(snapshot_minibatch.shape[1]/state_num)\n",
    "      output = model(snapshot_minibatch, step_i)\n",
    "      forward_count += 1\n",
    "      ground_truth = snapshot_minibatch.view(-1,model.state_num)\n",
    "      loss = criterion(output, ground_truth)\n",
    "      loss.backward()   \n",
    "      overall_loss += float(loss)\n",
    "      epoch_loss.append(float(loss))\n",
    "      optimizer.step()\n",
    "    \n",
    "    #model.network_push.increase_temperature(step_i, increase=1/100.0 * INCREASE_TEMPERATURE)\n",
    "\n",
    "    if (step_i == 0 or step_i % OUTPUT_EACH_X_EPOCHS == 0) or step_i == EPOCHNUM-1: # todo as hyper param\n",
    "      #if step_j == range(epochs)[-1]:\n",
    "      #model.network_push.collapse(stochastic=False)\n",
    "      #mean_test_acc, mean_test_loss = test_net(model, test_loader) #new\n",
    "      # dummy values to avoid div by zero \n",
    "      if forward_count == 0:\n",
    "         forward_count = 0.00001\n",
    "      if processed_samples == 0:\n",
    "        processed_samples = 0.00001\n",
    "      if USE_VALIDATIONSET:\n",
    "        mean_test_acc, mean_test_loss = test_net(model, test_snapshots) #old\n",
    "      else:\n",
    "        mean_test_acc, mean_test_loss = -1, -1\n",
    "      graph_edges = model.network_push.threshold_egelist()\n",
    "      weightpath_i = weightpath.format(int2str(step_i))\n",
    "      graphdist = graph_dist(ground_truth_graph, edgelist_to_graph(ground_truth_graph.number_of_nodes(), graph_edges))\n",
    "\n",
    "      record_i = {'epoch': step_i, 'loss_train': overall_loss/forward_count, 'loss_test': mean_test_loss, 'acc_test': mean_test_acc, 'graph_edges': str(graph_edges), 'weightpath': weightpath_i, 'graphdist': graphdist}\n",
    "      record_df = record_df.append(record_i, ignore_index=True)\n",
    "\n",
    "      plot_graph(model.network_push.threshold_nx(), 'trainplos_'+int2str(step_i), exp_name=exp_name, ground_truth=ground_truth_graph)\n",
    "\n",
    "      plot_dynamics('dyn_'+str(10000000000000+step_i), exp_name, model, state_num = state_num)\n",
    "      torch.save(model.state_dict(), weightpath_i)\n",
    "      mean_loss = list()\n",
    "      record_df['exp_name'] = exp_name\n",
    "      record_df.to_csv(HOME+exp_name+'/summary.csv', sep=';', index=False)\n",
    "      best = ''\n",
    "      mean_epoch_loss = np.mean(epoch_loss) if len(epoch_loss)>0 else -1.0\n",
    "\n",
    "      if mean_test_acc > best_mean_test_acc_sofar:\n",
    "        best_mean_test_acc_sofar = mean_test_acc\n",
    "        best = '  !'\n",
    "        decr_epoch_count = 0\n",
    "        best_graphdiff_sofar = graphdist\n",
    "      decr_epoch_count += 1\n",
    "      print('rolling loss: {:.6f}'.format(overall_loss/forward_count), '  epoch loss: {:.6f}'.format(mean_epoch_loss), '  test loss: {:.6f}'.format(mean_test_loss) , '  test acc: {:.6f}'.format(mean_test_acc) ,'graphdist: ',graphdist, '   mean time: {:.8f}'.format((time.time() - start_time)/processed_samples) + best)\n",
    "      model.network_push.increase_temperature(step_i)\n",
    "    \n",
    "      if EARLY_STOPPING and step_i > 50 and not False in [graph_edges == g for g in graph_epoch_lists]:\n",
    "        step_i = EPOCHNUM * 2\n",
    "        print('early stopping')\n",
    "        break\n",
    "      graph_epoch_lists = graph_epoch_lists[-9:] + [sorted(graph_edges)]\n",
    "    \n",
    "      # ploit\n",
    "      plt.clf()\n",
    "      plt.close()\n",
    "      sns.displot(model.network_push.weights.cpu().detach().flatten(), kde=True)\n",
    "      plt.savefig(weightpath.format(int2str(step_i))+'_w.png')\n",
    "      #if step_i >= 100*10:#40: #5 for atlas test or 10 when indiv layer is active\n",
    "      #  break\n",
    "        #print('finish') \n",
    "        #plot_graph(model.network_push.threshold_nx(), 'final_graph', exp_name=exp_name, ground_truth=ground_truth_graph, is_final=True)\n",
    "        #return record_df,  graphdist #best_graphdiff_sofar\n",
    "\n",
    "  print('finish training')\n",
    "  plot_graph(model.network_push.threshold_nx(), 'final_graph', exp_name=exp_name, ground_truth=ground_truth_graph, is_final=True)\n",
    "  record_df['exp_name'] = exp_name\n",
    "\n",
    "  return record_df, graphdist\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dense-senior",
   "metadata": {},
   "source": [
    "# Graph Generators\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unlike-adobe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# deterministic\n",
    "G_karate = nx.karate_club_graph()\n",
    "G_grid5x5 = nx.grid_2d_graph(5,5)\n",
    "G_grid10x10 = nx.grid_2d_graph(10,10)\n",
    "G_grid20x20 = nx.grid_2d_graph(20,20)\n",
    "G_lollipop = nx.lollipop_graph(10,10)\n",
    "G_circular_ladder = nx.circular_ladder_graph(100)\n",
    "\n",
    "# random\n",
    "G_erdos_small = nx.erdos_renyi_graph(25, 0.15, seed=43)\n",
    "G_erdos_small_wellcon = nx.erdos_renyi_graph(20, 0.35, seed=42)\n",
    "G_erdos = nx.erdos_renyi_graph(50, 0.1, seed=42)\n",
    "G_ba_small = nx.barabasi_albert_graph(25,3, seed=42)\n",
    "G_ba = nx.barabasi_albert_graph(100,4, seed=42)\n",
    "G_geom_small = nx.random_geometric_graph(50, 0.3, seed=42)\n",
    "G_geom = nx.random_geometric_graph(200, 0.125, seed=42)\n",
    "G_geom_large = nx.random_geometric_graph(400, 0.1, seed=42)\n",
    "G_wsn = nx.newman_watts_strogatz_graph(50, 4, 0.15, seed=42)\n",
    "\n",
    "# tiny\n",
    "G_erdos_tiny= nx.erdos_renyi_graph(10, 0.5, seed=42)\n",
    "G_lollipop_tiny = nx.lollipop_graph(5,5)\n",
    "G_circular_ladder_tiny = nx.circular_ladder_graph(10)\n",
    "G_grid_tiny = nx.grid_2d_graph(3,3)\n",
    "\n",
    "# use these graphs:\n",
    "ground_truth_graphset = {'G_erdos_small': G_erdos_small, 'G_geom': G_geom, 'G_grid10x10': G_grid10x10, 'G_wsn':G_wsn}\n",
    "ground_truth_graphset = {graph_name: clean_shuffle_graph(g) for graph_name, g in ground_truth_graphset.items()}\n",
    "\n",
    "#ground_atlas = [g for g in nx.graph_atlas_g() if g.number_of_nodes()==7 and nx.is_connected(g)]\n",
    "#ground_atlas = {'atlas'+str(100000+i):g for i,g in enumerate(ground_atlas)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "funny-lingerie",
   "metadata": {},
   "source": [
    "# Data Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "furnished-pencil",
   "metadata": {},
   "source": [
    "### Inverted Voter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "enhanced-memory",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_invvoter(G, noise=0.01):\n",
    "    A = [1., 0.]\n",
    "    B = [0., 1.]\n",
    "    steps = 1000 + random.choice(range(1000))\n",
    "    states = [random.choice([A, B]) for i in range(G.number_of_nodes())]\n",
    "    for _ in range(steps):\n",
    "        rates = np.zeros(G.number_of_nodes())\n",
    "        for n in range(G.number_of_nodes()):\n",
    "            rates[n] = len([n_j for n_j in G.neighbors(n) if states[n_j] == states[n]]) + noise\n",
    "            rates[n] = 1.0/rates[n] # numpy uses mean as rate param\n",
    "        jump_time = np.random.exponential(rates)\n",
    "        change_n = np.argmin(jump_time)\n",
    "        states[change_n] = A if states[change_n] == B else B     \n",
    "    return states"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "successful-renewal",
   "metadata": {},
   "source": [
    "### SIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "solved-westminster",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_sis(G, inf_rate=1.0, rec_rate=2.0, noise=0.1):\n",
    "    S = [1., 0.]\n",
    "    I = [0., 1.]\n",
    "    steps = 1000 + random.choice(range(1000))\n",
    "    states = [random.choice([S, I]) for i in range(G.number_of_nodes())]\n",
    "    for _ in range(steps):\n",
    "        rates = np.zeros(G.number_of_nodes())\n",
    "        for n in range(G.number_of_nodes()):\n",
    "            rates[n] = noise\n",
    "            if states[n] == I:\n",
    "                rates[n] += rec_rate\n",
    "            if states[n] == S:\n",
    "                rates[n] += inf_rate * len([n_j for n_j in G.neighbors(n) if states[n_j] == I])\n",
    "            rates[n] = 1.0/rates[n] # numpy uses mean as rate param\n",
    "        jump_time = np.random.exponential(rates)\n",
    "        change_n = np.argmin(jump_time)\n",
    "        states[change_n] = S if states[change_n] == I else I     \n",
    "    return states"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "elegant-disease",
   "metadata": {},
   "source": [
    "### Rock-Paper-Scissors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "instant-hardwood",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def gen_rps(G, change_rate=1.0, noise=0.1):\n",
    "    R = [1, 0, 0]\n",
    "    P = [0, 1, 0]\n",
    "    S = [0, 0, 1]\n",
    "    steps = 1000 + random.choice(range(1000))\n",
    "    states = [random.choice([R, P, S]) for i in range(G.number_of_nodes())]\n",
    "    for _ in range(steps):\n",
    "        rates = np.zeros(G.number_of_nodes())\n",
    "        for n in range(G.number_of_nodes()):\n",
    "            rates[n] = noise\n",
    "            if states[n] == R:\n",
    "                rates[n] += change_rate * len([n_j for n_j in G.neighbors(n) if states[n_j] == P]) # paper wins against rock\n",
    "            if states[n] == P:\n",
    "                rates[n] += change_rate * len([n_j for n_j in G.neighbors(n) if states[n_j] == S])\n",
    "            if states[n] == S:\n",
    "                rates[n] += change_rate * len([n_j for n_j in G.neighbors(n) if states[n_j] == R])\n",
    "            rates[n] = 1.0/rates[n] # numpy uses mean as rate param\n",
    "        jump_time = np.random.exponential(rates)\n",
    "        change_n = np.argmin(jump_time)\n",
    "        if states[change_n] == R:\n",
    "            states[change_n] = P\n",
    "        elif states[change_n] == P:\n",
    "            states[change_n] = S\n",
    "        elif states[change_n] == S:\n",
    "            states[change_n] = R\n",
    "    return states"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "invalid-maple",
   "metadata": {},
   "source": [
    "### Forest Fire Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "greek-rogers",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_forestfire(G, growth_rate=1.0, lightning_rate=0.1, firespread_rate = 2.0, fireextinct_rate = 2.0, noise=0.01):\n",
    "    E = [1, 0, 0] # empty\n",
    "    T = [0, 1, 0] # tree\n",
    "    L = [0, 0, 1] # fire\n",
    "    steps = 1000 + random.choice(range(1000))\n",
    "    states = [random.choice([E, T, L]) for i in range(G.number_of_nodes())]\n",
    "    for _ in range(steps):\n",
    "        rates = np.zeros(G.number_of_nodes())\n",
    "        for n in range(G.number_of_nodes()):\n",
    "            rates[n] = noise\n",
    "            if states[n] == E:\n",
    "                rates[n] += growth_rate\n",
    "            if states[n] == T:\n",
    "                rates[n] += firespread_rate if len([n_j for n_j in G.neighbors(n) if states[n_j]==L])>0 else 0.0\n",
    "                rates[n] += lightning_rate\n",
    "            if states[n] == L:\n",
    "                rates[n] += fireextinct_rate\n",
    "            rates[n] = 1.0/rates[n] # numpy uses mean as rate param\n",
    "        jump_time = np.random.exponential(rates)\n",
    "        change_n = np.argmin(jump_time)\n",
    "        if states[change_n] == E:\n",
    "            states[change_n] = T\n",
    "        elif states[change_n] == T:\n",
    "            states[change_n] = L\n",
    "        elif states[change_n] == L:\n",
    "            states[change_n] = E\n",
    "    return states"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dutch-communications",
   "metadata": {},
   "source": [
    "### Coupled Map Lattice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "duplicate-punch",
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve_cmp(G, s=0.1, r=3.57):\n",
    "  def f_map(x): return r*x*(1.0-x)\n",
    "\n",
    "  states = [random.random() for j in range(G.number_of_nodes())]\n",
    "  steps = 100 + random.choice(range(100))\n",
    "\n",
    "  for _ in range(steps):\n",
    "    new_states = list(states)\n",
    "    for n in G.nodes():\n",
    "      v = states[n]\n",
    "      neig = list(G.neighbors(n))\n",
    "      new_states[n] = (1.0-s) * f_map(v) + s/len(neig) * np.sum([f_map(states[n_j]) for n_j in neig])\n",
    "    states = list(new_states)\n",
    "  \n",
    "  return states\n",
    "\n",
    "def gen_cmp(G, s=0.1, r=3.57, levels=2):\n",
    "    def get_bin(value, levels):\n",
    "        z = np.linspace(0,1,levels+1)[1:]\n",
    "        return np.sum(z < value)\n",
    "\n",
    "    def one_hot(value, levels):\n",
    "        vec = [0]*levels\n",
    "        vec[value] = 1\n",
    "        return vec\n",
    "    states = solve_cmp(G, s, r)\n",
    "    states = [get_bin(states[j], levels) for j in range(G.number_of_nodes())]\n",
    "    states = [one_hot(s, levels) for s in states]\n",
    "    return states\n",
    "\n",
    "def gen_cmp10(G, s=0.1, r=3.57):\n",
    "    return gen_cmp(G, s=s, r=r, levels=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "anonymous-harris",
   "metadata": {},
   "source": [
    "### Majority-Flip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acting-gasoline",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_majorityflip(G, change_rate=1.0, noise=0.01):\n",
    "    A = [1., 0.]\n",
    "    D = [0., 1.]\n",
    "    steps = 1000 + random.choice(range(1000))\n",
    "    states = [random.choice([A, D]) for i in range(G.number_of_nodes())]\n",
    "    for _ in range(steps):\n",
    "        rates = np.zeros(G.number_of_nodes())\n",
    "        for n in range(G.number_of_nodes()):\n",
    "            alive_neighbors =  len([n_j for n_j in G.neighbors(n) if states[n_j] == A])\n",
    "            dead_neighbors =  len([n_j for n_j in G.neighbors(n) if states[n_j] == D])\n",
    "            alive_frac = alive_neighbors/(alive_neighbors+dead_neighbors)\n",
    "            rates[n] = noise\n",
    "            if states[n] == A and (alive_frac < 0.2 or alive_frac > 0.8):\n",
    "                rates[n] += change_rate\n",
    "            if states[n] == D and (alive_frac < 0.3 or alive_frac > 0.7):\n",
    "                rates[n] +=change_rate\n",
    "            rates[n] = 1.0/rates[n] # numpy uses mean as rate param\n",
    "        jump_time = np.random.exponential(rates)\n",
    "        change_n = np.argmin(jump_time)\n",
    "        states[change_n] = A if states[change_n] == D else D     \n",
    "    return states"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "offshore-genetics",
   "metadata": {},
   "source": [
    "### Summary Dynamics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cathedral-lender",
   "metadata": {},
   "outputs": [],
   "source": [
    "# name -> method, number_of_states    \n",
    "ground_truth_dynamicsset = {\n",
    "    'inv_voter': (gen_invvoter, 2),\n",
    "    'sis': (gen_sis, 2),\n",
    "    'rock_paper_scissors': (gen_rps, 3),\n",
    "    'forest_fire': (gen_forestfire, 3),\n",
    "    'cmp': (gen_cmp10, 10),\n",
    "    'majorityflip': (gen_majorityflip, 2)\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "neither-colon",
   "metadata": {},
   "source": [
    "# Putting Everything Together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "terminal-passport",
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment(exp_name, G, dynamics, snapshots_num = 10000, state_num=2, overwrite=False, init=False, snapshots = None):\n",
    "  gen_folders(exp_name, home=HOME)\n",
    "  time.sleep(0.1)\n",
    "  try:\n",
    "    assert(False)\n",
    "    assert(not overwrite)\n",
    "    G = nx.read_gml(HOME+exp_name+'/ground_truth_graph.gml') # stupid nx reads nodes as str and not as int\n",
    "    node_mapping = {n: int(n) for n in G.nodes()} # TODO test\n",
    "    G = nx.relabel_nodes(G, node_mapping)\n",
    "    print('found graph')\n",
    "  except:\n",
    "    print('write ground truth graph: ',HOME+exp_name+'/ground_truth_graph.gml')\n",
    "    nx.write_gml(G, HOME+exp_name+'/ground_truth_graph.gml')\n",
    "    nx.write_edgelist(G, HOME+exp_name+'/ground_truth_graph.edgelist', data=False)\n",
    "  \n",
    "  assert(set(range(G.number_of_nodes())) == set(G.nodes()))\n",
    "  assert(nx.is_connected(G))\n",
    "\n",
    "  if snapshots is None:\n",
    "    try:\n",
    "      assert(not overwrite)\n",
    "      snapshots = read_snapshots(HOME+exp_name+'/snapshots.txt')\n",
    "      print('found snapshot data, using this instead. Length is: ', len(snapshots))\n",
    "    except:\n",
    "      print('write snapshots')\n",
    "      snapshots = [None for _ in range(snapshots_num)]\n",
    "      time.sleep(0.1)\n",
    "    # 1)\n",
    "      for i in tqdm(range(snapshots_num)):\n",
    "        snapshots[i] = dynamics(G)\n",
    "    # 2)\n",
    "    #  #snapshots = [dynamics(G) for _ in range(snapshots_num)]\n",
    "    # 3)\n",
    "      #from joblib import Parallel, delayed\n",
    "      #snapshots = Parallel(n_jobs=16)(delayed(dynamics)(G) for i in range(snapshots_num))\n",
    "      write_snapshots(snapshots, HOME+exp_name+'/snapshots.txt')\n",
    "      plot_snapshots(G, snapshots, exp_name)\n",
    "  \n",
    "  #data = snapshots_to_data(snapshots) # seed\n",
    "\n",
    "  print('start training')\n",
    "  init_graph = None\n",
    "  if 'networkx' in str(type(init)):\n",
    "    init_graph = nx_to_adj(init)\n",
    "  elif 'bool' in str(type(init)) and init:\n",
    "    init_graph = get_baseline_graph(snapshots)\n",
    "    #init_graph = nx_to_adj(init)\n",
    "  record, score = start_training(exp_name, snapshots, ground_truth_graph = G, state_num=state_num, init_graph=init_graph) #also seed for test train split ...\n",
    "  return score, snapshots, record\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "express-europe",
   "metadata": {},
   "source": [
    "# Run Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "casual-indianapolis",
   "metadata": {},
   "source": [
    "### Exp 0: Test Random Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gothic-gossip",
   "metadata": {},
   "outputs": [],
   "source": [
    "def example_run(state_num=2):\n",
    "  #HOME = './'\n",
    "  G = G_ba_small\n",
    "  G = clean_shuffle_graph(G)\n",
    "\n",
    "  score, snapshots, record = experiment(\"SIS_5k\", G, gen_sis, snapshots_num = 3000, state_num=state_num, init=False) \n",
    "  bs = baseline_summary(snapshots, G)\n",
    "  for name, (dist, time) in bs.items():\n",
    "    print(name, ':   ' ,dist)\n",
    "\n",
    " \n",
    "EPOCHNUM=20*100\n",
    "USE_VALIDATIONSET=False\n",
    "INDIVIDUAL_INPUT = False\n",
    "INDIVIDUAL_OUTPUT = True\n",
    "RANDOM_TEMPERATURE = True\n",
    "INCREASE_TEMPERATURE = 1.0\n",
    "\n",
    "OUTPUT_EACH_X_EPOCHS = 100\n",
    "MINIBATCH_SIZE = 100\n",
    "\n",
    "\n",
    "example_run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "opposite-northwest",
   "metadata": {},
   "source": [
    "### Exp 1: Identifiability\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inside-prospect",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_atlas(dynamic_func, exp_name, indiv_out=True, state_num=2, individual_input= False):\n",
    "  global GRAPH_DROPOUT, EARLY_STOPPING, TRAIN_NETWORK, MINI_BATCHSIZE, USESHIFT, TEMPERATURE, LEARNING_RATE, INIT_GRAPH_SHIFT, HOME,INDIVIDUAL_OUTPUT, EPOCHNUM, INCREASE_TEMPERATURE, USE_VALIDATIONSET, INDIVIDUAL_INPUT\n",
    "  TRAIN_NETWORK = False\n",
    "  MINI_BATCHSIZE = 1000\n",
    "  USESHIFT = True\n",
    "  TEMPERATURE = 20.0 \n",
    "  INCREASE_TEMPERATURE = 0.0\n",
    "  graph_id = 426 # this is int(len(ground_atlas)/2)\n",
    "  LEARNING_RATE =  0.001\n",
    "  INIT_GRAPH_SHIFT = 10\n",
    "  os.system('mkdir EXP1')\n",
    "  HOME = './EXP1/exp1_{}/'.format(exp_name)\n",
    "  INDIVIDUAL_OUTPUT = indiv_out\n",
    "  EPOCHNUM= 6 * 100\n",
    "  USE_VALIDATIONSET=False\n",
    "  INDIVIDUAL_INPUT = individual_input\n",
    "  EARLY_STOPPING = False\n",
    "  GRAPH_DROPOUT = False\n",
    "    \n",
    "  if INDIVIDUAL_OUTPUT:\n",
    "    EPOCHNUM = 8\n",
    "\n",
    "  print(\"run atlas: \", exp_name)\n",
    "  try:\n",
    "    picklepath = HOME+'atlas_exp.pickle'\n",
    "    return_values = pickle.load(open(picklepath, \"rb\" ))\n",
    "    print('read atlas results: ',picklepath)\n",
    "    return return_values\n",
    "  except:\n",
    "    pass\n",
    "\n",
    "  atlas_summary =  {'exp_name': list(), 'epoch': list(), 'loss_train': list(), 'loss_test': list(), 'acc_test': list(), 'graph_edges': list(), 'weightpath': list(), 'graphdist': list()}\n",
    "\n",
    "  ground_atlas = create_all_graphs(node_num=5, check_if_connected=True)\n",
    "\n",
    "  G = nx.generators.small.bull_graph()\n",
    "  G = clean_shuffle_graph(G)\n",
    "  samples = 40000  # sample num should be 40000\n",
    "\n",
    "  score, snapshots, record = experiment('atlasAll_'+str(graph_id)+'_gt', G, dynamic_func, snapshots_num = samples, state_num=state_num, init=G, snapshots=None)\n",
    "  for key in atlas_summary:\n",
    "      if key == 'acc_test':\n",
    "          atlas_summary[key].append(np.max(list(record[key])[1:]))\n",
    "      elif 'loss' in key:\n",
    "        atlas_summary[key].append(np.min(list(record[key])[1:]))\n",
    "      else:\n",
    "        atlas_summary[key].append(list(record[key])[-1])\n",
    "\n",
    "  atlas_random = list(enumerate(ground_atlas))\n",
    "  random.seed(42)\n",
    "  random.shuffle(atlas_random)\n",
    "  random.seed(None)\n",
    "  counter = 0\n",
    "\n",
    "  for i, G_i in atlas_random:\n",
    "    counter = counter + 1\n",
    "    score, snapshots, record = experiment('atlasAll_'+str(i), G, gen_sis, snapshots_num = None, state_num=state_num, init=G_i, snapshots=snapshots)\n",
    "    for key in atlas_summary:\n",
    "      if key == 'acc_test':\n",
    "          atlas_summary[key].append(np.max(list(record[key])[1:]))\n",
    "      elif 'loss' in key:\n",
    "        atlas_summary[key].append(np.min(list(record[key])[1:]))\n",
    "      else:\n",
    "        atlas_summary[key].append(list(record[key])[-1])\n",
    "    if counter % 20 == 0:\n",
    "      atlas_summary_df = pd.DataFrame(atlas_summary)\n",
    "      atlas_summary_df.to_csv(HOME+'atlas_summary.csv')\n",
    "      plot_atlas(HOME+'atlas_summary.pdf', atlas_summary_df)\n",
    "      print(counter)\n",
    "    #print(atlas_summary)\n",
    "\n",
    "  atlas_summary_df = pd.DataFrame(atlas_summary)\n",
    "  atlas_summary_df.to_csv(HOME+'atlas_summary.csv')\n",
    "  print(atlas_summary_df)\n",
    "  plot_atlas(HOME+'atlas_summary.pdf', atlas_summary_df)\n",
    "  pickle.dump(atlas_summary_df, open(picklepath, \"wb\" ))\n",
    "  HOME = 'drive/MyDrive/colab/NeuralGraphInference/'\n",
    "  return atlas_summary_df\n",
    "\n",
    "for dynamicsname, dyn_func_state_num in ground_truth_dynamicsset.items():\n",
    "    dyn_func, state_num = dyn_func_state_num\n",
    "    test_atlas(dyn_func, dynamicsname+\"_exp1\", False, state_num=state_num)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "extreme-patch",
   "metadata": {},
   "source": [
    "### Exp 2: Reconstruction Accuracy w.r.t. epoch num/sample num/node num\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "complimentary-charles",
   "metadata": {},
   "outputs": [],
   "source": [
    "def exp_samplenum(dynamic_func, exp_name, state_num=2, random_temperature=False, indiv_out=False):\n",
    "    global EPOCHNUM, GRAPH_DROPOUT, USE_VALIDATIONSET, HOME, MINIBATCH_SIZE, INCREASE_TEMPERATURE, RANDOM_TEMPERATURE, OUTPUT_EACH_X_EPOCHS, EARLY_STOPPING, INDIVIDUAL_OUTPUT\n",
    "    INCREASE_TEMPERATURE = 1.0\n",
    "    RANDOM_TEMPERATURE = False\n",
    "    SAMPLE_NUM = 50000\n",
    "    OUTPUT_EACH_X_EPOCHS = 50\n",
    "    MINIBATCH_SIZE = 100\n",
    "    USE_VALIDATIONSET = False\n",
    "    INDIVIDUAL_OUTPUT = True \n",
    "    INDIVIDUAL_INPUT = False\n",
    "    GRAPH_DROPOUT = True\n",
    "    EPOCHNUM = 40*100\n",
    "    EARLY_STOPPING = False\n",
    "    \n",
    "    os.system('mkdir EXP2')\n",
    "    HOME = './EXP2/exp2_{}/'.format(exp_name)\n",
    "    \n",
    "    exp_summary = None\n",
    "    \n",
    "    print(\"run Exp2: \", exp_name)\n",
    "    \n",
    "    try:\n",
    "        picklepath = HOME+'EXP2data.pickle'\n",
    "        return_values = pickle.load(open(picklepath, \"rb\" ))\n",
    "        print('read EXP2 results: ',picklepath)\n",
    "        return return_values\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    counter = 0\n",
    "    for grid_dim in [5,7,10]:\n",
    "        G = nx.grid_2d_graph(grid_dim,grid_dim)\n",
    "        G = nx.convert_node_labels_to_integers(G)\n",
    "        for sample_num in [100, 1000, 10*1000]:\n",
    "            MINIBATCH_SIZE = int(sample_num/10)\n",
    "            for run_id in range(5):\n",
    "                counter = counter + 1\n",
    "                exp_name_i = exp_name+'_'+str(grid_dim).zfill(10)+'_'+str(sample_num).zfill(10)+'_'+str(run_id)\n",
    "                score, snapshots, record = experiment(exp_name_i, G, dynamic_func, snapshots_num = sample_num, state_num=state_num)\n",
    "                record['sample_num'] = sample_num\n",
    "                record['grid_dim'] = grid_dim\n",
    "                record['run_id'] = run_id\n",
    "                if exp_summary is None:\n",
    "                    exp_summary = record\n",
    "                else:\n",
    "                    exp_summary = exp_summary.append(record, ignore_index=True)\n",
    "                if counter % 2 == 0:\n",
    "                    exp_summary.to_csv(HOME+'EXP2_summary.csv')\n",
    "                    plot_exp2(HOME+'EXP2_summary.pdf', exp_summary)\n",
    "\n",
    "    exp_summary.to_csv(HOME+'EXP2_summary.csv')\n",
    "    plot_exp2(HOME+'EXP2_summary.pdf', exp_summary)\n",
    "    print(exp_summary)\n",
    "    pickle.dump(exp_summary, open(picklepath, \"wb\" ))\n",
    "    return exp_summary\n",
    "\n",
    "USE_VALIDATIONSET=False\n",
    "for dynamicsname, dyn_func_state_num in ground_truth_dynamicsset.items():\n",
    "    dyn_func, state_num = dyn_func_state_num\n",
    "    exp_samplenum(dyn_func, dynamicsname+\"_exp2\", state_num=state_num) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "designed-newman",
   "metadata": {},
   "source": [
    "### Exp 3: GBN vs Baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "technological-maintenance",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_baseline(snapshots, graph, exp_name_i):\n",
    "    try:\n",
    "        picklepath = HOME+exp_name_i+'/baseline.pickle'\n",
    "        bl = pickle.load(open(picklepath, \"rb\" ))\n",
    "        return bl\n",
    "    except:\n",
    "        picklepath = HOME+exp_name_i+'/baseline.pickle'\n",
    "        bl = baseline_summary(snapshots, graph)\n",
    "        pickle.dump(bl, open(picklepath, \"wb\" ))\n",
    "        return bl\n",
    "\n",
    "\n",
    "def exp_GBCvsBaseline(exp_name, random_temperature=False, individual_output=True, individual_input=False):\n",
    "    global EPOCHNUM, GRAPH_DROPOUT, USE_VALIDATIONSET, HOME, MINIBATCH_SIZE, INCREASE_TEMPERATURE, RANDOM_TEMPERATURE, OUTPUT_EACH_X_EPOCHS, INDIVIDUAL_INPUT\n",
    "    os.system('mkdir EXP3')\n",
    "    HOME = './EXP3/exp3_{}/'.format(exp_name)\n",
    "    INCREASE_TEMPERATURE = 1.0\n",
    "    RANDOM_TEMPERATURE = random_temperature\n",
    "    SAMPLE_NUM = 50000 # should be 50k\n",
    "    OUTPUT_EACH_X_EPOCHS = 50\n",
    "    MINIBATCH_SIZE = 100\n",
    "    USE_VALIDATIONSET = False\n",
    "    INDIVIDUAL_OUTPUT = individual_output \n",
    "    INDIVIDUAL_INPUT = False\n",
    "    GRAPH_DROPOUT = True\n",
    "    EPOCHNUM = 100*100\n",
    "    \n",
    "    exp_summary = None\n",
    "    exp_summary_short = None\n",
    "    \n",
    "    print(\"run Exp3: \", exp_name)\n",
    "    \n",
    "    try:\n",
    "        picklepath = HOME+'EXP3data.pickle'\n",
    "        return_values = pickle.load(open(picklepath, \"rb\" ))\n",
    "        print('read EXP3 results: ',picklepath)\n",
    "        return return_values\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "\n",
    "    exp_id = 0\n",
    "    for dynamicsname, dyn_func_state_num in ground_truth_dynamicsset.items():\n",
    "        dyn_func, state_num = dyn_func_state_num\n",
    "        for graphname, graph in ground_truth_graphset.items():\n",
    "            for run_id in range(1):\n",
    "                exp_id += 1\n",
    "                exp_name_i = exp_name+'_'+str(dynamicsname)+'_'+str(graphname)+'_'+str(run_id).zfill(3)\n",
    "                \n",
    "                start_time = time.time() # measure time of complete reconstruction\n",
    "                score, snapshots, record = experiment(exp_name_i, graph, dyn_func, snapshots_num = SAMPLE_NUM, state_num=state_num)\n",
    "                time_elapsed = time.time() - start_time\n",
    "                record['dynamicsname'] = dynamicsname\n",
    "                record['graphname'] = graphname\n",
    "                record['run_id'] = run_id\n",
    "                record['exp_id'] = exp_id\n",
    "                record['time_elapsed'] = time_elapsed\n",
    "                final_graph_dist =  list(record.graphdist)[-1]\n",
    "                record['final_graphdiff'] = final_graph_dist\n",
    "\n",
    "                bl = load_baseline(snapshots, graph, exp_name_i)\n",
    "                for name, (value, elapsed_time) in bl.items():\n",
    "                    record['Baseline_'+name] = value\n",
    "                    record['Baseline_'+name+'_time'] = elapsed_time\n",
    "                    \n",
    "                if exp_summary is None:\n",
    "                    exp_summary = record\n",
    "                else:\n",
    "                    exp_summary = exp_summary.append(record, ignore_index=True)\n",
    "                    \n",
    "                if exp_summary_short is None:\n",
    "                    df_i = {'method': 'out_method', 'node_num': [graph.number_of_nodes()], 'time': [elapsed_time], 'graphdist': [final_graph_dist], 'final_graph': ['None'], 'graphname': [graphname], 'dynamicsname':[dynamicsname]}\n",
    "                    print(df_i)\n",
    "                    exp_summary_short = pd.DataFrame(df_i)\n",
    "                else:\n",
    "                    df_i = {'method': 'out_method', 'time':elapsed_time, 'graphdist': final_graph_dist, 'final_graph': 'None', 'graphname': graphname, 'dynamicsname':dynamicsname,  'node_num': graph.number_of_nodes()}\n",
    "                    exp_summary_short = exp_summary_short.append(df_i, ignore_index=True)\n",
    "                for name, (value, elapsed_time) in bl.items():\n",
    "                    df_i = {'method': 'Baseline_'+name, 'node_num': graph.number_of_nodes(), 'time':elapsed_time, 'graphdist': value, 'final_graph': 'None', 'graphname': graphname, 'dynamicsname':dynamicsname}\n",
    "                    exp_summary_short = exp_summary_short.append(df_i, ignore_index=True)\n",
    "                    \n",
    "    \n",
    "                if exp_id % 1 == 0:\n",
    "                    exp_summary.to_csv(HOME+'EXP3_summary.csv')\n",
    "                    exp_summary_short.to_csv(HOME+'EXP3_summaryshort.csv')\n",
    "                    plot_exp3(exp_summary_short, HOME+'EXP3_{}.jpg')\n",
    "    \n",
    "    exp_summary.to_csv(HOME+'EXP3_summary.csv')\n",
    "\n",
    "    \n",
    "\n",
    "exp_GBCvsBaseline('run1')\n",
    "# there will be some pandas warnings that can be ignored\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
